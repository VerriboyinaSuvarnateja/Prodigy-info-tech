{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f8240c5-ca66-48bc-9d2c-c65d548cc6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf5b029efb964cc3ba374cd9f9e22e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 8\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#Importing\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "#Creating a n object for  GPT2Tokenizer. \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#Loading the textfile as datset\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": \"my_dataset.txt\"})\n",
    "\n",
    "#Assigning the Token ids to all the Tokens.\n",
    "def tokenize(batch):\n",
    "    tokens = tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "#Removing the text columns\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5c37efa-7a16-4830-b41a-42391048c280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=3.726496696472168, metrics={'train_runtime': 32.6759, 'train_samples_per_second': 0.734, 'train_steps_per_second': 0.184, 'total_flos': 1567752192000.0, 'train_loss': 3.726496696472168, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing the actual model\n",
    "from transformers import GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "\n",
    "#Model is created\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "#Giving the training Arguments.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"no\",\n",
    ")\n",
    "\n",
    "#Training the Model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7f9b320-136f-4ac5-8fcd-4dc3bd534e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stars began to be more and the stars in the room, the stars stars began to form.\n",
      "\n",
      "\n",
      "The star was very very thin and it was very very very thin.\n",
      "\n",
      "\n",
      "The stars were looking in the middle of the stars.\n",
      "\n",
      "\n",
      "The stars from\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "#Generating the teaxt\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "output = generator(\"The stars began to\", max_new_tokens=50)\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c52c992-ae07-4c30-8e29-ce132a6b0976",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
